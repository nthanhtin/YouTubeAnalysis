{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Spark Initialization and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Word Count\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\r\n",
      "OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_252-b09)\r\n",
      "OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.252-b09, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------------------------------+\n",
      "|        creator_name|userid|                              comment|\n",
      "+--------------------+------+-------------------------------------+\n",
      "|        Doug The Pug|  87.0|                 I shared this to ...|\n",
      "|        Doug The Pug|  87.0|                   Super cute  ðŸ˜€ðŸ•ðŸ¶|\n",
      "|         bulletproof| 530.0|                 stop saying get e...|\n",
      "|       Meu ZoolÃ³gico| 670.0|                 Tenho uma jiboia ...|\n",
      "|              ojatro|1031.0|                 I wanna see what ...|\n",
      "|     Tingle Triggers|1212.0|                 Well shit now Im ...|\n",
      "|Hope For Paws - O...|1806.0|                 when I saw the en...|\n",
      "|Hope For Paws - O...|2036.0|                 Holy crap. That i...|\n",
      "|          Life Story|2637.0|æ­¦å™¨ã¯ã‚¯ã‚¨ã‚¹ãƒˆã§è²°ãˆã‚‹ã‚“ã˜ã‚ƒãªã„ã‚“...|\n",
      "|       Brian Barczyk|2698.0|                 Call the teddy Larry|\n",
      "+--------------------+------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import *\n",
    "from time import *\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "df=spark.read.csv(\"youtube/animals_comments.csv\",\n",
    "                      inferSchema=True,\n",
    "                      header=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creator_name', 'string'), ('userid', 'double'), ('comment', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_rest= df.randomSplit([0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+--------------------+-------+------+-----+\n",
      "|creator_name| userid|             comment|dog_cat|no_pet|label|\n",
      "+------------+-------+--------------------+-------+------+-----+\n",
      "|        null| 9224.0|So many ignorant ...|  false|  true|  0.0|\n",
      "|        null|14478.0|copter pilot no g...|  false|  true|  0.0|\n",
      "|        null|28126.0|That guy will los...|  false|  true|  0.0|\n",
      "|        null|28324.0|           ggvvvvvvv|  false|  true|  0.0|\n",
      "|        null|33544.0|Nice buy Islamaba...|  false|  true|  0.0|\n",
      "|        null|33661.0|Ð”Ð°Ð¼Ð¸ Ð¸ Ð“Ð¾ÑÐ¿Ð¾Ð´Ð° Ð¿Ñ€...|  false|  true|  0.0|\n",
      "|        null|36985.0|Vert nice eye can...|  false|  true|  0.0|\n",
      "|        null|37343.0| Alec is my favorite|  false| false|  0.0|\n",
      "|        null|37407.0|Long time fan her...|  false| false|  0.0|\n",
      "|        null|40081.0|Stefan is... brok...|  false|  true|  0.0|\n",
      "|        null|52789.0|HELL YEAH now wer...|  false|  true|  0.0|\n",
      "|        null|53007.0|bruh he slapet hi...|  false|  true|  0.0|\n",
      "|        null|55762.0|Love that show Im...|  false| false|  0.0|\n",
      "|        null|58377.0|Oh My poor girl K...|  false|  true|  0.0|\n",
      "|        null|59574.0|Yo Jay can you re...|  false|  true|  0.0|\n",
      "|        null|59745.0|love your reactio...|  false|  true|  0.0|\n",
      "|        null|62434.0|           Emma dies|  false|  true|  0.0|\n",
      "|        null|67979.0|               owsem|  false|  true|  0.0|\n",
      "|        null|74711.0|           Mashallah|  false|  true|  0.0|\n",
      "|        null|79376.0|I loove this new ...|  false|  true|  0.0|\n",
      "+------------+-------+--------------------+-------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "cond = (df[\"comment\"].like(\"%my dog%\") \n",
    "        | df[\"comment\"].like(\"%I have a dog%\") \n",
    "        | df[\"comment\"].like(\"%my dogs%\") \n",
    "        | df[\"comment\"].like(\"%I have dog%\")\n",
    "        | df[\"comment\"].like(\"%my cat%\") \n",
    "        | df[\"comment\"].like(\"%my cats%\") \n",
    "        | df[\"comment\"].like(\"%I have a cat%\") \n",
    "        | df[\"comment\"].like(\"%I have cat%\") \n",
    "        | df[\"comment\"].like(\"%my puppy%\") \n",
    "        | df[\"comment\"].like(\"%my puppies%\") \n",
    "        | df[\"comment\"].like(\"%my kitty%\") \n",
    "        | df[\"comment\"].like(\"%my kitties%\") \n",
    "        | df[\"comment\"].like(\"%I have a kitty%\") \n",
    "        | df[\"comment\"].like(\"%I have kitties%\") \n",
    "        | df[\"comment\"].like(\"%I have a puppy%\") \n",
    "        | df[\"comment\"].like(\"%I have puppies%\"))\n",
    "\n",
    "df_clean = df.withColumn('dog_cat',  cond)\n",
    "\n",
    "# find user do not have \n",
    "df_clean = df_clean.withColumn('no_pet', ~df_clean[\"comment\"].like(\"%my%\") & ~df_clean[\"comment\"].like(\"%have%\")) \n",
    "df_clean = df_clean.withColumn('label', col(\"dog_cat\").cast(IntegerType()).cast('double'))\n",
    "\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colume in df_clean.columns:\n",
    "    df_clean=df_clean.filter(df_clean[colume].isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.data preprocessing and tokenizing\n",
    "tokenizing is the process of dividing a string of written language into its component words.\n",
    "Stop words are words that appear frequently in the document, but do not carry too much meaning. They should not be considered in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define a list of stop words or use default list\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() \n",
    "\n",
    "# Display some of the stop words\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"text\", pattern=\"\\\\W\")\n",
    "df_clean= regexTokenizer.transform(df_clean)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+\n",
      "|        creator_name|   userid|             comment|dog_cat|no_pet|label|                text|     vector_no_stopw|\n",
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 106860.0|Niggas in the sam...|  false|  true|  0.0|[niggas, in, the,...|[niggas, position...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 254431.0|How u Wuga World ...|  false|  true|  0.0|[how, u, wuga, wo...|[u, wuga, world, ...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 273476.0|Look up BLACK FAM...|  false|  true|  0.0|[look, up, black,...|[look, black, fam...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 479125.0|Whole concept dop...|  false|  true|  0.0|[whole, concept, ...|[whole, concept, ...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 864514.0|Alright I see u B...|  false|  true|  0.0|[alright, i, see,...|[alright, see, u,...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 895169.0|t-shirt from h&m ...|  false|  true|  0.0|[t, shirt, from, ...| [shirt, h, m, lmao]|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1000411.0|   Dopeeeee mbk shit|  false|  true|  0.0|[dopeeeee, mbk, s...|[dopeeeee, mbk, s...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1060242.0|1400 gang shitevi...|  false| false|  0.0|[1400, gang, shit...|[1400, gang, shit...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1271064.0|shit got me goose...|  false|  true|  0.0|[shit, got, me, g...|[shit, got, goose...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1434236.0|beauty bar las ve...|  false|  true|  0.0|[beauty, bar, las...|[beauty, bar, las...|\n",
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.setInputCol(\"text\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "df_clean = remover.transform(df_clean)\n",
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Porter stemming algorithm (or â€˜Porter stemmerâ€™) is a process for removing the commoner morphological and inflexional endings from words in English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+\n",
      "|        creator_name|   userid|             comment|dog_cat|no_pet|label|                text|     vector_no_stopw|      vector_stemmed|\n",
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 106860.0|Niggas in the sam...|  false|  true|  0.0|[niggas, in, the,...|[niggas, position...|[nigga, posit, ye...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 254431.0|How u Wuga World ...|  false|  true|  0.0|[how, u, wuga, wo...|[u, wuga, world, ...|[wuga, world, don...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 273476.0|Look up BLACK FAM...|  false|  true|  0.0|[look, up, black,...|[look, black, fam...|[look, black, fam...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 479125.0|Whole concept dop...|  false|  true|  0.0|[whole, concept, ...|[whole, concept, ...|[whole, concept, ...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 864514.0|Alright I see u B...|  false|  true|  0.0|[alright, i, see,...|[alright, see, u,...|[alright, see, bi...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 895169.0|t-shirt from h&m ...|  false|  true|  0.0|[t, shirt, from, ...| [shirt, h, m, lmao]|       [shirt, lmao]|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1000411.0|   Dopeeeee mbk shit|  false|  true|  0.0|[dopeeeee, mbk, s...|[dopeeeee, mbk, s...|[dopeeee, mbk, shit]|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1060242.0|1400 gang shitevi...|  false| false|  0.0|[1400, gang, shit...|[1400, gang, shit...|[1400, gang, shit...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1271064.0|shit got me goose...|  false|  true|  0.0|[shit, got, me, g...|[shit, got, goose...|[shit, got, goose...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1434236.0|beauty bar las ve...|  false|  true|  0.0|[beauty, bar, las...|[beauty, bar, las...|[beauti, bar, veg...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1644687.0|black a bitch but...|  false|  true|  0.0|[black, a, bitch,...|[black, bitch, de...|[black, bitch, de...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1673999.0|You had $40 in yo...|  false|  true|  0.0|[you, had, 40, in...|[40, paypal, acco...|[paypal, account,...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|2284744.0|Lol This is a Cas...|  false|  true|  0.0|[lol, this, is, a...|[lol, cash, money...|[lol, cash, money...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|2447103.0|      billy ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥|  false|  true|  0.0|             [billy]|             [billy]|             [billi]|\n",
      "|   1-800-Flowers.com| 560584.0|i love flowers as...|  false|  true|  0.0|[i, love, flowers...|[love, flowers, g...|[love, flower, gift]|\n",
      "|        100% Madison| 302879.0|1k madisonnn cong...|  false|  true|  0.0|[1k, madisonnn, c...|[1k, madisonnn, c...|[madisonnn, congr...|\n",
      "|        100% Madison| 821666.0|               SQUAD|  false|  true|  0.0|             [squad]|             [squad]|             [squad]|\n",
      "|        100% Madison|1614747.0|ive never seen or...|  false|  true|  0.0|[ive, never, seen...|[ive, never, seen...|[ive, never, seen...|\n",
      "|        100% Madison|1643154.0|so many questions...|  false|  true|  0.0|[so, many, questi...|[many, questions,...|[mani, question, ...|\n",
      "|        100% Madison|1934896.0|         GR8 CHAPTER|  false|  true|  0.0|      [gr8, chapter]|      [gr8, chapter]|      [gr8, chapter]|\n",
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Instantiate stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Create stemmer python function\n",
    "def stem(in_vec):\n",
    "    out_vec = []\n",
    "    for t in in_vec:\n",
    "        t_stem = stemmer.stem(t)\n",
    "        if len(t_stem) > 2:\n",
    "            out_vec.append(t_stem)       \n",
    "    return out_vec\n",
    "\n",
    "# Create user defined function for stemming with return type Array<String>\n",
    "from pyspark.sql.types import *\n",
    "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
    "\n",
    "# Create new column with vectors containing the stemmed tokens \n",
    "df_clean = df_clean.withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n",
    "\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|        creator_name|   userid|             comment|dog_cat|no_pet|label|                text|     vector_no_stopw|      vector_stemmed|                  tf|\n",
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 106860.0|Niggas in the sam...|  false|  true|  0.0|[niggas, in, the,...|[niggas, position...|[nigga, posit, ye...|(200,[26,55,87,99...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 254431.0|How u Wuga World ...|  false|  true|  0.0|[how, u, wuga, wo...|[u, wuga, world, ...|[wuga, world, don...|(200,[15,58,104,1...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 273476.0|Look up BLACK FAM...|  false|  true|  0.0|[look, up, black,...|[look, black, fam...|[look, black, fam...|(200,[18,28,62,65...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 479125.0|Whole concept dop...|  false|  true|  0.0|[whole, concept, ...|[whole, concept, ...|[whole, concept, ...|(200,[40,85,103,1...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 864514.0|Alright I see u B...|  false|  true|  0.0|[alright, i, see,...|[alright, see, u,...|[alright, see, bi...|(200,[115,125,141...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...| 895169.0|t-shirt from h&m ...|  false|  true|  0.0|[t, shirt, from, ...| [shirt, h, m, lmao]|       [shirt, lmao]|(200,[20,126],[1....|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1000411.0|   Dopeeeee mbk shit|  false|  true|  0.0|[dopeeeee, mbk, s...|[dopeeeee, mbk, s...|[dopeeee, mbk, shit]|(200,[4,12,60],[1...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1060242.0|1400 gang shitevi...|  false| false|  0.0|[1400, gang, shit...|[1400, gang, shit...|[1400, gang, shit...|(200,[47,78,83,10...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1271064.0|shit got me goose...|  false|  true|  0.0|[shit, got, me, g...|[shit, got, goose...|[shit, got, goose...|(200,[7,44,60],[1...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1434236.0|beauty bar las ve...|  false|  true|  0.0|[beauty, bar, las...|[beauty, bar, las...|[beauti, bar, veg...|(200,[51,67,73,82...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1644687.0|black a bitch but...|  false|  true|  0.0|[black, a, bitch,...|[black, bitch, de...|[black, bitch, de...|(200,[125,149,197...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|1673999.0|You had $40 in yo...|  false|  true|  0.0|[you, had, 40, in...|[40, paypal, acco...|[paypal, account,...|(200,[1,13,18,31,...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|2284744.0|Lol This is a Cas...|  false|  true|  0.0|[lol, this, is, a...|[lol, cash, money...|[lol, cash, money...|(200,[30,42,63,85...|\n",
      "|#CameraLordâ„¢ â€¢ Ko...|2447103.0|      billy ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥|  false|  true|  0.0|             [billy]|             [billy]|             [billi]|   (200,[141],[1.0])|\n",
      "|   1-800-Flowers.com| 560584.0|i love flowers as...|  false|  true|  0.0|[i, love, flowers...|[love, flowers, g...|[love, flower, gift]|(200,[13,40,158],...|\n",
      "|        100% Madison| 302879.0|1k madisonnn cong...|  false|  true|  0.0|[1k, madisonnn, c...|[1k, madisonnn, c...|[madisonnn, congr...|(200,[21,29,42,53...|\n",
      "|        100% Madison| 821666.0|               SQUAD|  false|  true|  0.0|             [squad]|             [squad]|             [squad]|   (200,[112],[1.0])|\n",
      "|        100% Madison|1614747.0|ive never seen or...|  false|  true|  0.0|[ive, never, seen...|[ive, never, seen...|[ive, never, seen...|(200,[17,42,50,53...|\n",
      "|        100% Madison|1643154.0|so many questions...|  false|  true|  0.0|[so, many, questi...|[many, questions,...|[mani, question, ...|(200,[0,9,24,40,9...|\n",
      "|        100% Madison|1934896.0|         GR8 CHAPTER|  false|  true|  0.0|      [gr8, chapter]|      [gr8, chapter]|      [gr8, chapter]|(200,[90,121],[1....|\n",
      "+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import col,size,count,when,isnan\n",
    "from pyspark.sql import *\n",
    "from functools import reduce\n",
    "\n",
    "df_clean.na.drop()\n",
    "#Maps a sequence of terms to their term frequencies using the hashing.\n",
    "hashingTF = HashingTF(inputCol=\"vector_stemmed\", outputCol=\"tf\", numFeatures=200)\n",
    "featurizedData = hashingTF.transform(df_clean)\n",
    "featurizedData.na.drop()\n",
    "\n",
    "featurizedData.withColumn('userid', col('userid').cast('float').cast(IntegerType()))\n",
    "\n",
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF is an Estimator which is fit on a dataset and produces an IDFModel. \n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(200,[26,55,87,99...|\n",
      "|  0.0|(200,[15,58,104,1...|\n",
      "|  0.0|(200,[18,28,62,65...|\n",
      "|  0.0|(200,[40,85,103,1...|\n",
      "|  0.0|(200,[115,125,141...|\n",
      "|  0.0|(200,[20,126],[3....|\n",
      "|  0.0|(200,[4,12,60],[3...|\n",
      "|  0.0|(200,[47,78,83,10...|\n",
      "|  0.0|(200,[7,44,60],[3...|\n",
      "|  0.0|(200,[51,67,73,82...|\n",
      "|  0.0|(200,[125,149,197...|\n",
      "|  0.0|(200,[1,13,18,31,...|\n",
      "|  0.0|(200,[30,42,63,85...|\n",
      "|  0.0|(200,[141],[3.535...|\n",
      "|  0.0|(200,[13,40,158],...|\n",
      "|  0.0|(200,[21,29,42,53...|\n",
      "|  0.0|(200,[112],[2.710...|\n",
      "|  0.0|(200,[17,42,50,53...|\n",
      "|  0.0|(200,[0,9,24,40,9...|\n",
      "|  0.0|(200,[90,121],[3....|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 3124\n",
      "testing size 796\n"
     ]
    }
   ],
   "source": [
    "pet = rescaledData.filter(\"label=1.0\")\n",
    "pet_train, pet_test = pet.randomSplit([0.8, 0.2])\n",
    "nopet = rescaledData.filter(\"label=0.0\")\n",
    "sampleRatio = float(pet.count()) / float(nopet.count())\n",
    "sample_nopet = nopet.sample(False, sampleRatio)\n",
    "df_sample = pet.unionAll(sample_nopet)\n",
    "sample_nopet_train, sample_nopet_test = sample_nopet.randomSplit([0.8, 0.2])\n",
    "\n",
    "df_train = pet_train.unionAll(sample_nopet_train)\n",
    "df_test = pet_test.unionAll(sample_nopet_test)\n",
    "print ('training size',df_train.count())\n",
    "print ('testing size',df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "lr = LogisticRegression(maxIter=10,featuresCol='features', labelCol='label')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01,0.1]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "crossval = CrossValidator(estimator = lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "\n",
    "cvModel = crossval.fit(df_train)\n",
    "best_model = cvModel.bestModel\n",
    "trainingSummary = best_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"youtube/\"\n",
    "\n",
    "best_model.save(path + 'best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set areaUnderROC: 0.9667893698761792\n",
      "Testing set areaUnderROC 0.9453214396348439\n",
      "Training set accuracy: 0.9231754161331626\n",
      "Testing set accuracy 0.8932160804020101\n"
     ]
    }
   ],
   "source": [
    "prediction_train = best_model.transform(df_train)\n",
    "prediction_test = best_model.transform(df_test)\n",
    "accuracy_train = prediction_train.filter(prediction_train.label == prediction_train.prediction).count()/float(df_train.count())\n",
    "accuracy_test = prediction_test.filter(prediction_test.label == prediction_test.prediction).count()/float(df_test.count())\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(evaluator.evaluate(prediction_train)))\n",
    "print('Testing set areaUnderROC ' + str(evaluator.evaluate(prediction_test)))\n",
    "print('Training set accuracy: ' + str(accuracy_train))\n",
    "print('Testing set accuracy ' + str(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classify All The Users\n",
    "We can now apply the cat/dog classifiers to all the other users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_pet_owner : 31964\n",
      "total_population : 240075\n",
      "pet_owner_ratio : 0.1331417265437884\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model.transform(rescaledData)\n",
    "\n",
    "total_pet_owner = prediction.filter(\"prediction = 1.0\").count()\n",
    "total_population = df.select(\"userid\").distinct().count()\n",
    "pet_owner_ratio = float(total_pet_owner)/float(total_population)\n",
    "print('total_pet_owner :',total_pet_owner)\n",
    "print('total_population :',total_population)\n",
    "print('pet_owner_ratio :',pet_owner_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get insigts of Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7442\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[win, happi, home, long, adopt, vegan, right, ass, one, shelbi, store, swim, cho, colleg, chi, eagl, chewi, releas, evil, record]      |\n",
      "|[get, hors, back, guy, tank, littl, day, put, cute, ride, mom, show, water, treat, come, poor, never, way, cut, start]                 |\n",
      "|[one, video, love, get, make, dont, time, know, dog, see, want, peopl, use, got, like, also, anim, year, work, realli]                 |\n",
      "|[kitten, hola, comment, half, miss, counter, dan, bulli, box, surgeri, other, race, stitch, nose, dude, view, bitten, het, leaf, fed]  |\n",
      "|[dog, thank, snake, help, pleas, say, beauti, think, god, dont, sound, hurt, know, play, food, leg, male, she, save, funni]            |\n",
      "|[anim, peopl, fish, live, que, cri, turtl, pack, brave, con, wilder, someon, ive, made, por, may, theyr, wild, betta, una]             |\n",
      "|[name, pit, birthday, rabbit, black, boy, bull, bunni, white, one, shark, parent, collar, omg, part, mother, happen, gold, bird, child]|\n",
      "|[like, look, lol, big, sonic, spider, game, giveaway, girl, vid, hous, max, sorri, hair, differ, play, bottl, feel, ear, love]         |\n",
      "|[cat, love, babi, great, hope, die, boo, need, old, huski, eye, place, gohan, much, monkey, outsid, crazi, year, month, meow]          |\n",
      "|[watch, tri, amaz, robin, vlog, mean, two, wish, hamster, find, head, life, fox, nice, fuck, top, light, children, song, feel]         |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "\n",
    "pet_owner = prediction.filter(\"prediction = 1.0\").select('userid','vector_stemmed')\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"vector_stemmed\", outputCol=\"features\",\n",
    "                     minTF=2, # minium number of times a word must appear in a document\n",
    "                     minDF=4) # minimun number of documents a word must appear in\n",
    "\n",
    "countVectorModel = cv.fit(pet_owner)\n",
    "\n",
    "countVectors = (countVectorModel\n",
    "                .transform(pet_owner)\n",
    "                .select(\"userid\", \"features\").cache())\n",
    "\n",
    "print(len(countVectorModel.vocabulary))  # how many documents, vocab size\n",
    "\n",
    "numTopics = 10 # number of topics\n",
    "\n",
    "lda = LDA(k = numTopics,\n",
    "          maxIter = 50 # number of iterations\n",
    "          )\n",
    "\n",
    "ldaModel = lda.fit(countVectors)\n",
    "\n",
    "\n",
    "# Print topics and top-weighted terms\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=20)\n",
    "vocabArray = countVectorModel.vocabulary\n",
    "\n",
    "ListOfIndexToWords = udf(lambda wl: list([vocabArray[w] for w in wl]))\n",
    "FormatNumbers = udf(lambda nl: [\"{:1.4f}\".format(x) for x in nl])\n",
    "\n",
    "topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identify Creators With Cat And Dog Owners In The Audience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|        creator_name|count(DISTINCT userid)|\n",
      "+--------------------+----------------------+\n",
      "|    Brave Wilderness|                  2605|\n",
      "|            The Dodo|                  2315|\n",
      "|        Robin Seplut|                  1714|\n",
      "|       Brian Barczyk|                  1252|\n",
      "|  Taylor Nicole Dean|                  1202|\n",
      "|Hope For Paws - O...|                  1175|\n",
      "|           Vet Ranch|                   940|\n",
      "|    Cole & Marmalade|                   889|\n",
      "|     Gohan The Husky|                   773|\n",
      "|     Viktor Larkhill|                   758|\n",
      "|Gone to the Snow ...|                   641|\n",
      "|   Talking Kitty Cat|                   592|\n",
      "|          stacyvlogs|                   499|\n",
      "|        Paws Channel|                   461|\n",
      "|Zak Georges Dog T...|                   386|\n",
      "|            ViralHog|                   325|\n",
      "|  Think Like A Horse|                   288|\n",
      "|         Info Marvel|                   287|\n",
      "|    SlideShow ForFun|                   263|\n",
      "|  The Pet Collective|                   254|\n",
      "+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "tmp = prediction.filter(\"prediction = 1.0\")\n",
    "tmp.groupBy('creator_name').agg(countDistinct('userid')).sort('count(DISTINCT userid)',ascending= False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
